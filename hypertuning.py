# -*- coding: utf-8 -*-
"""Exp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JjJRaP2Ln0MApG-c5nTsUrTaJr41BJOe
"""

# Loading the Google drive/folder into Colab
from google.colab import drive
drive.mount('/content/drive')
#!ls -lh /content/drive/My\ Drive/ADNI_komplett

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Import data from Excel sheet
import pandas as pd
df = pd.read_excel('/content/drive/My Drive/ADNI combined.xlsx', sheet_name='sample')
#df = pd.read_excel('C:/Users/u006377/Desktop/ADNI_komplett/ADNI combined.xlsx', sheet_name='sample')
#print(df)
sid = df['RID']
grp = df['Group at scan date (1=CN, 2=EMCI, 3=LMCI, 4=AD, 5=SMC)']
age = df['Age at scan']
sex = df['Sex (1=female)']
tiv = df['TIV']
field = df['MRI_Field_Strength']
grpbin = (grp > 1) # 1=CN, ...

# Scan for nifti file names
import glob
dataAD = sorted(glob.glob('/content/drive/My Drive/AD/*.nii.gz'))
dataLMCI = sorted(glob.glob('/content/drive/My Drive/LMCI/*.nii.gz'))
dataCN = sorted(glob.glob('/content/drive/My Drive/CN/*.nii.gz'))
dataFiles = dataAD + dataLMCI + dataCN

####################
#checking with only ad and cn
#dataFiles = dataAD  + dataCN

############################

numfiles = len(dataFiles)
print('Found ', str(numfiles), ' nifti files')

# Match covariate information
import re
debug = False
cov_idx = [-1] * numfiles # list; array: np.full((numfiles, 1), -1, dtype=int)
print('Matching covariates for loaded files ...')
for i,id in enumerate(sid):
  p = [j for j,x in enumerate(dataFiles) if re.search('_%04d_' % id, x)] # translate ID numbers to four-digit numbers, get both index and filename
  if len(p)==0:
    if debug: print('Did not find %04d' % id) # did not find Excel sheet subject ID in loaded file selection
  else:
    if debug: print('Found %04d in %s: %s' % (id, p[0], dataFiles[p[0]]))
    cov_idx[p[0]] = i # store Excel index i for data file index p[0]
print('Checking for scans not found in Excel sheet: ', sum(x<0 for x in cov_idx))

labels = pd.DataFrame({'Group':grpbin}).iloc[cov_idx, :]
grps = pd.DataFrame({'Group':grp, 'RID':sid}).iloc[cov_idx, :]

# Load residualized data from disk
import h5py
import numpy as np
from pandas import DataFrame
#hf = h5py.File('/content/drive/My Drive/residuals_wb.hdf5', 'r')

############
hf = h5py.File('/content/drive/My Drive/wb_orig+label.hdf5', 'r')
#hf = h5py.File('/content/drive/My Drive/32slice_orig+label.hdf5', 'r')
#############

hf.keys # read keys
labels = np.array(hf.get('labels')) # note: was of data frame type before
images = np.array(hf.get('images'))
hf.close()
print(images.shape)

# Display a single scan
from matplotlib import pyplot as plt
#import numpy as np
test_img = images[0, :,:,:, 0]
ma = np.max(test_img)
mi = np.min(test_img)
test_img = (test_img - mi) / (ma - mi) # normalising to (0-1) and then normalising to 0 mean and 1 std
#test_img = (test_img - test_img.mean())/test_img.std() # normalizing by mean and sd
print('displaying image ', dataFiles[0])
for i in range(test_img.shape[2]):
  if (i % 10 == 0): # only display each fifth slice
    plt.figure()
    a = test_img[:,:,i]
    plt.imshow(a, cmap='gray')

# define function for simple data augmentation (translation of 2 vx in each x/y/z direction)
# and L/R flipping
import numpy as np
def simpleshift(arr, num, axis, fill_value=0): # adapted from https://stackoverflow.com/a/42642326
  result = np.empty_like(arr)
  if (axis==1):
    if num > 0:
        result[:, :num, :, :, :] = fill_value
        result[:, num:, :, :, :] = arr[:, :-num, :, :, :]
    elif num < 0:
        result[:, num:, :, :, :] = fill_value
        result[:, :num, :, :, :] = arr[:, -num:, :, :, :]
    else:
        result[:] = arr
    return result
  elif (axis==2):
    if num > 0:
        result[:, :, :num, :, :] = fill_value
        result[:, :, num:, :, :] = arr[:, :, :-num, :, :]
    elif num < 0:
        result[:, :, num:, :, :] = fill_value
        result[:, :, :num, :, :] = arr[:, :, -num:, :, :]
    else:
        result[:] = arr
    return result
  elif (axis==3):
    if num > 0:
        result[:, :, :, :num, :] = fill_value
        result[:, :, :, num:, :] = arr[:, :, :, :-num, :]
    elif num < 0:
        result[:, :, :, num:, :] = fill_value
        result[:, :, :, :num, :] = arr[:, :, :, -num:, :]
    else:
        result[:] = arr
    return result
  else:
    return None

import tensorflow as tf
import time
#from tensorflow.keras.callbacks import TensorBoard
import os
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto(
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)
#     device_count = {'GPU': 1}
)
config.gpu_options.allow_growth = True
session = tf.Session(config=config)
set_session(session)

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0" # model will be trained on GPU 0
from time import gmtime, strftime

print(strftime("Start time of exp May%d %H:%M:%S", gmtime()))

from keras.callbacks import CSVLogger
import datetime
batch_sizes = [64] #16

#lr_n_decay = [[0.0001,0],[0.0001,0.001],[0.0001,0.0001],[0.001,0],[0.001,0.01],[0.001,0.001]]
#lr_n_decay = [[0.001,0.01]]

#kernelsizes = [20] #20 # varying only the kenel size of 1st conv layer
#dropout = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]
#dropout = [0.6,0.7,0.8]

#kernelsizes = [16] # varying only the kenel size of 1st conv layer
#lr_n_decay = [[0.0001,0],[0.0001,0.001],[0.0001,0.002],[0.0001,0.003],[0.0001,0.004],[0.0001,0.005],[0.0001,0.006]]

# Split data into training/validation and holdout test data
from sklearn.model_selection import StratifiedKFold,train_test_split
import numpy as np
from keras.utils import to_categorical
import gc
from keras.models import load_model

labels = to_categorical(np.asarray(labels)) # use grps to access original labels
acc_test, auc_test, auc_AD, auc_MCI = [], [], [], []
#skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)

pl_loss=[]
pl_val_loss=[]
pl_acc=[]
pl_val_acc=[]

#idx = np.asarray(range(numfiles))
#tmp_Y = np.asarray(grps.iloc[:, 0])

#tmp_idX, test_idX = train_test_split(idx,test_size=0.1,random_state=2) # Splitting into test and tmp set , tmp set to be further split into train and valid
#tmp_Y = np.asarray(grps.iloc[tmp_idX, 0])
#%matplotlib inline
#for ks in kernelsizes:
for bs in batch_sizes:
  pl_loss=[]
  pl_val_loss=[]
  pl_acc=[]
  pl_val_acc=[]
  cnt=1

  #NAME = "{}-batch_size-{}".format(batch_size, int(time.time()))
  #tboard_log_dir = os.path.join("logs",NAME)
  #tensorboard = TensorBoard(log_dir = tboard_log_dir)
  #print(batch_size)
  #for tmp_idX, test_idX in skf.split(X=images, y=grps.iloc[:, 0]): # split data as tenfold stratified cross-validation
      #tmp_Y = np.asarray(grps.iloc[tmp_idX, 0])
      #train_idX,valid_idX,train_Y,valid_Y = train_test_split(tmp_idX, tmp_Y, test_size=0.1, stratify=tmp_Y, random_state=2)

  skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # do 10 fold xval

  for train_idX,valid_idX in skf.split(X=images, y=grps.iloc[:, 0]): # split data as tenfold stratified cross-validation
      #print('valid index : ',valid_idX)
      #print('train index: ',train_idX)
      #print('test index : ',test_idX)
      #print('fold = ',skf.get_n_splits(tmp_idX,tmp_Y))
      
      #print('Distribution of diagnoses in training data: [1=CN, 3=LMCI, 4=AD]')
      #print(grps.iloc[train_idX, :].Group.value_counts())
      testgrps = grps.iloc[valid_idX, :]   ############### changed from test_idX to valid_idX
      #print(testgrps) # prints diagnosis and RID
      #print('Distribution of diagnoses in holdout test data: [1=CN, 3=LMCI, 4=AD]')
      #print(testgrps.Group.value_counts())
      
      # Setup CNN model
      
      #os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
      #os.environ["CUDA_VISIBLE_DEVICES"]="0" # model will be trained on GPU 0
      
      import keras
      from keras import layers
      from keras.layers.normalization import BatchNormalization
      from keras import models
      from keras.optimizers import Adam

      
      #3D convnet model
      input_shape= images.shape[1:] #(84, 88, 111, 1)
      
      model = models.Sequential()
      # Convolution Layers
      model.add(layers.Conv3D(5, (3, 3, 3), padding='same', input_shape=input_shape, data_format='channels_last'))
      model.add(layers.BatchNormalization())
      model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))
      model.add(layers.Activation('relu'))
      #model.add(layers.Dropout(rate = 0.3))
      
      model.add(layers.Conv3D(5, (3, 3, 3), padding='same'))
      model.add(layers.BatchNormalization())
      model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))
      model.add(layers.Activation('relu'))
      #model.add(layers.Dropout(rate = 0.3))
      
      model.add(layers.Conv3D(5, (3, 3, 3), padding='same'))
      model.add(layers.BatchNormalization())
      model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))
      model.add(layers.Activation('relu'))
      #model.add(layers.Dropout(rate = 0.3))
      """
      model.add(layers.Conv3D(5, (3, 3, 3), padding='same'))
      model.add(layers.BatchNormalization())
      model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))
      model.add(layers.Activation('relu'))
      #model.add(layers.Dropout(rate = kr))

      model.add(layers.Conv3D(20, (3, 3, 3), padding='same'))
      model.add(layers.BatchNormalization())
      model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))
      model.add(layers.Activation('relu'))
      """
      model.add(layers.Dropout(rate = 0.4))

      # FC layer
      model.add(layers.Flatten())
      #model.add(layers.Dropout(rate = 0.4))
      #model.add(layers.Dense(20, activation='relu'))
      #model.add(layers.Dropout(rate = 0.4))
      #model.add(layers.Dense(20, activation='relu'))
      #model.add(layers.Dropout(rate = 0.4))
      model.add(layers.Dense(2, activation='softmax'))
      model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = 0.001 ,decay=0.01), metrics=['accuracy'])
      #model.summary()   Adam(lr = l_rate,decay = decay_fac)
      
      
      traindat = images[train_idX, :]
      train_Y = labels[train_idX, :]
      valdat = images[valid_idX, :]
      valid_Y = labels[valid_idX, :]
      testdat = images[valid_idX, :]   ############### changed from test_idX to valid_idX
      test_Y = labels[valid_idX, :]    ############### changed from test_idX to valid_idX
      #del images # try to free some memory
      
      """
      traindat = np.concatenate((traindat,
                                simpleshift(traindat, -2, 1), simpleshift(traindat, 2, 1), # move 2 vx around x-axis
                                simpleshift(traindat, -2, 2), simpleshift(traindat, 2, 2), # move 2 vx around y-axis
                                simpleshift(traindat, -2, 3), simpleshift(traindat, 2, 3)), # move 2 vx around z-axis  np.flip(traindat, axis=2)), # L/R flip
                                axis=0)
      
      #traindat = np.concatenate((traindat,np.flip(traindat, axis=2)),axis=0) # L/R flip
      
      train_label = np.concatenate((#train_Y, train_Y, train_Y, train_Y, train_Y, train_Y, train_Y,
                                    train_Y, train_Y, train_Y, train_Y, train_Y, train_Y, train_Y),
                                    axis=0)
      """
      #"""
      traindat = np.concatenate((traindat,simpleshift(traindat, -2, 1)),axis=0)
      #traindat = np.concatenate((traindat,np.flip(traindat, axis=2)),axis=0) # L/R flip
      
      train_label = np.concatenate((train_Y, train_Y),axis=0)
      #"""

      ##########
      #train_label = train_Y
      #traindat = np.concatenate((traindat,np.flip(traindat, axis=2)),axis=0) # L/R flip
      #train_label = np.concatenate((train_Y, train_Y),axis=0)
      ######### 
      
      # Fit model to training data
      #batch_size = 64
      epochs = 80 # validation accuracy doesn't increase any more after XX? epochs
      

      #NAME = "{}-kernel_size-bs=8-lr_dec=0.001_0.01-{}".format(ks, int(time.time()))
      #tboard_log_dir = os.path.join("logs_varied_conv_kernel",NAME)
      #tensorboard = TensorBoard(log_dir = tboard_log_dir)
      
      #csv_logger = CSVLogger('log-{}filters_5CNN-blocks.log'.format(ks),append = True)
      #csv_logger = CSVLogger('log-baseline-drop-after each block-3times data_aug+with residual_-{}batchsize.log'.format(bs),append = True)
      csv_logger = CSVLogger('wb_3cnn_no-residual-2x-dataug.log',append = True)


      hist = model.fit(traindat, train_label, batch_size=bs,epochs=epochs, verbose=0,validation_data=(valdat, valid_Y),callbacks=[csv_logger])#,tensorboard])#,callbacks=[tensorboard])
      
      #Display models statistics
      from matplotlib import pyplot as plt
      
      loss = hist.history['loss']
      pl_loss.append(loss)
      val_loss = hist.history['val_loss']
      pl_val_loss.append(val_loss)
      acc = hist.history['accuracy']
      pl_acc.append(acc)
      val_acc = hist.history['val_accuracy']
      pl_val_acc.append(val_acc)
      epochsr = range(epochs)
      #plt.figure()
      #plt.plot(epochsr, loss, 'b', label='Training loss')
      #plt.plot(epochsr, val_loss, 'r', label='Validation loss')
      #plt.title('Training and validation loss')
      #plt.xlabel('epochs')
      #plt.ylabel('loss')

      #plt.legend()

      #plt.show()
      #plt.figure()
      #plt.plot(epochsr, acc, 'b', label='Training acc')
      #plt.plot(epochsr, val_acc, 'r', label='Validation acc')
      #plt.title('Accuracy')
      #plt.xlabel('epochs')
      #plt.ylabel('accuracy')
      #plt.legend()

      #plt.show()
      
      print('batch size =',bs)
      #print('lr and decay =',lr_de)
      #print('kernel size = ',ks)
      #print('dropout = ',kr)
      mymodel = hist.model


      mymodel.save("baseline_fold{}.hdf5".format(cnt))
      mymodel = load_model("baseline_fold{}.hdf5".format(cnt))


      #mymodel.save("32slice_model_{}filters_5-CNN_blocks_fold{}.hdf5".format(ks,cnt))
      #mymodel = load_model("32slice_model_{}filters_5-CNN_blocks_fold{}.hdf5".format(ks,cnt))
      print('End of fold {}'.format(cnt))
      cnt=cnt+1
      
      # Calculate accuracy for holdout test data
      scores = mymodel.evaluate(testdat, test_Y) #, verbose=0
      print("Test %s: %.2f%%" % (mymodel.metrics_names[1], scores[1]*100))
      

      
      acc_test.append(scores[1]*100)
      
      # calculate area under the curve
      # AUC as optimization function during training: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras
      from sklearn.metrics import roc_curve, auc
      pred = mymodel.predict(testdat)
      
      
      fpr = dict()
      tpr = dict()
      roc_auc = dict()
      for i in range(2): # classes dummy vector: 0 - CN, 1 - MCI/AD
          fpr[i], tpr[i], _ = roc_curve(test_Y[:, i], pred[:,i])
          roc_auc[i] = auc(fpr[i], tpr[i])

      #Plot of a ROC curve for a specific class
      #plt.figure()
      #plt.plot(fpr[1], tpr[1], color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc[1])
      #plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
      #plt.xlim([0.0, 1.0])
      #plt.ylim([0.0, 1.05])
      #plt.xlabel('False Positive Rate')
      #plt.ylabel('True Positive Rate')
      #plt.title('Receiver operating characteristic')
      #plt.legend(loc="lower right")
      #plt.show()
      auc_test.append(roc_auc[1])
      
      testgrps = grps.iloc[valid_idX, :]   ############### changed from test_idX to valid_idX
      #print(testgrps) # prints diagnosis and RID

      #print('Distribution of diagnoses in holdout test data: [1=CN, 3=LMCI, 4=AD]')
      #print(testgrps.Group.value_counts())

      # redo AUC for binary comparison: AD vs. HC and MCI vs. HC
      for i in [3,4]:
          grpi = np.equal(testgrps.Group.to_numpy(dtype=np.int), np.ones((testgrps.shape[0],), dtype=np.int)*i)
          grp1 = np.equal(testgrps.Group.to_numpy(dtype=np.int), np.ones((testgrps.shape[0],), dtype=np.int))
          grpidx = np.logical_or(grpi, grp1)
          fpr[i], tpr[i], _ = roc_curve(test_Y[grpidx, 1], pred[grpidx, 1])
          roc_auc[i] = auc(fpr[i], tpr[i])

      print('AUC for MCI vs. CN = %0.2f' % roc_auc[3])
      print('AUC for AD vs. CN = %0.2f' % roc_auc[4])
      auc_AD.append(roc_auc[4])
      auc_MCI.append(roc_auc[3])
      #hist = None
      #mymodel = None
      #gc.collect()
      #gc.collect()
  print(strftime("Time after end of 10x val %d %H:%M:%S", gmtime()))
      
  plt.plot(epochsr, np.transpose(pl_val_acc),alpha=0.5)
  mean = np.mean(pl_val_acc, axis=0)
  standard_dev = np.std(pl_val_acc, axis=0)
  print('Mean val acc ',mean)
  print('Std val acc',standard_dev)
  plt.plot(epochsr,mean,linewidth=4,label='mean validation acc')
  #plt.title('Mean validation accuracy for {} filters'.format(ks))
  plt.title('Mean validation accuracy')
  plt.xlabel('epochs')
  plt.ylabel('val_acc')
  plt.fill_between(epochsr, mean-standard_dev, mean+standard_dev, alpha = 0.5)
  plt.legend()

  plt.show()

  plt.plot(epochsr, np.transpose(pl_val_loss),alpha=0.5)
  mean = np.mean(pl_val_loss, axis=0)
  standard_dev = np.std(pl_val_loss, axis=0)
  print('Mean val loss',mean)
  print('Std val loss',standard_dev)
  plt.plot(epochsr,mean,linewidth=4,label='mean validation loss')
  #plt.title('Mean validation loss for {} filters'.format(ks))
  plt.title('Mean validation loss')
  plt.fill_between(epochsr, mean-standard_dev, mean+standard_dev, alpha = 0.5)
  plt.xlabel('epochs')
  plt.ylabel('val_loss')
  plt.legend()

  plt.show()
  """
  plt.plot(epochsr, np.transpose(pl_acc),alpha=0.5)
  mean = np.mean(pl_acc, axis=0)
  standard_dev = np.std(pl_acc, axis=0)
  print('Mean training acc',mean)
  print('Std training acc',standard_dev)
  plt.plot(epochsr,mean,linewidth=4,label='mean training acc')
  #plt.title('Mean training accuracy for {} filters'.format(ks))
  plt.title('Mean training accuracy')
  plt.xlabel('epochs')
  plt.ylabel('accuracy')
  plt.fill_between(epochsr, mean-standard_dev, mean+standard_dev, alpha = 0.5)
  plt.legend()

  plt.show()

  plt.plot(epochsr, np.transpose(pl_loss),alpha=0.5)
  mean = np.mean(pl_loss, axis=0)
  standard_dev = np.std(pl_loss, axis=0)
  print('Mean training loss',mean)
  print('Std training loss',standard_dev)
  plt.plot(epochsr,mean,linewidth=4,label='mean training loss')
  plt.title('Mean training loss for {} filters'.format(ks))
  plt.xlabel('epochs')
  plt.ylabel('loss')
  plt.fill_between(epochsr, mean-standard_dev, mean+standard_dev, alpha = 0.5)
  plt.legend()
  
  plt.show()
  #mymodel.summary()
  """

  #print('loss = ',pl_loss)
  #print('val_loss = ',pl_val_loss)
  #print('acc = ',pl_acc)
  #print('val_acc = ',pl_val_acc)
  # print model performance summary
  from statistics import mean,stdev
  print('Acc for all test data = %0.1f +/- %0.1f' % (mean(acc_test), stdev(acc_test)))
  print('AUC for all test data = %0.3f +/- %0.3f' % (mean(auc_test), stdev(auc_test)))
  print('AUC for MCI vs. CN = %0.3f +/- %0.3f' % (mean(auc_MCI), stdev(auc_MCI)))
  print('AUC for AD vs. CN = %0.3f +/- %0.3f' % (mean(auc_AD), stdev(auc_AD)))

#strftime("Start time of exp May%d %H:%M:%S", gmtime())
#print(strftime("%d %H:%M:%S", gmtime()))

traindat.shape





# Load CNN model from disk
from keras.models import load_model
#mymodel = load_model('/content/32slice_model_5CNN_layers_20_1FC-ADCN_wo-dataaug_fold5.hdf5')
mymodel = load_model('/content/32slice_model_baseline_5filters_3CNN_1FC_fold5.hdf5')
mymodel.summary()

!pip install innvestigate

#Load original images from disk
import h5py
import numpy as np
hf = h5py.File('/content/drive/My Drive/ADNI_komplett/orig_images.hdf5', 'r')
#hf = h5py.File('/content/drive/My Drive/orig_ADCN.hdf5', 'r')
hf.keys # read keys
images_orig = np.array(hf.get('images'))
hf.close()

#can get the desired sub by choosing the fold of valid idx containing sub but the print results are wrong

#valid_idX = np.asarray([4,  34,  35,  37,  41,  44,  70,  84,  85,  94, 100, 104, 109, 112, 121, 140, 143, 167, 183, 191, 198, 218, 226, 229, 258, 260, 278, 284, 295, 302, 316, 320, 334, 336, 341, 350, 355, 357, 359, 365, 401, 414, 417, 427])

#valid_idX = [22,  32,  64,  66,  67,  72,  82,  83,  86,  99, 111, 113, 119, 123, 132, 134, 151, 165,
# 195, 208, 212, 214, 216, 220, 224, 227, 228, 230, 237, 238, 243, 251 ,266, 269, 306, 318,
# 347 ,368 ,371 ,374 ,379 ,389 ,397 ,425]

valid_idX = [ 22,  32,  64,  66,  67,  72,  82,  83,  86,  99, 111, 113, 119, 123, 132, 134, 151, 165,
 194, 203, 208, 209 ,214 ,219 ,227, 232 ,254, 279 ,281 ,288, 303, 308 ,327 ,346 ,351 ,353,
 362, 368 ,392 ,405 ,414 ,425 ,427 ,446 ,447 ,463 ,467 ,470 ,503 ,511 ,521 ,525 ,536 ,556,
 558 ,565 ,573 ,575 ,582 ,583 ,603 ,605, 627 ,642 ,646 ,652]

import innvestigate
import innvestigate.utils as iutils
import numpy as np
from matplotlib import pyplot as plt

model_wo_softmax = iutils.keras.graph.model_wo_softmax(mymodel)
#model_wo_softmax.summary()

subj_idx = 9 # good visualizations for subjects idx 4 (AD), 5 (AD), 6 (LMCI), 8 (AD), 10 (LMCI), 27 (CN)
test_img = testdat[subj_idx]
test_orig = testdat_orig[subj_idx]
print('test image for subject of binary group: %d' % test_Y[subj_idx, 1]) # first col will indicate CN, second col indicates MCI/AD
print('test image for subject of ADNI diagnosis: %d [1-CN, 3-LMCI, 4-AD]' % testgrps.Group.to_numpy(dtype=np.int)[subj_idx])
print('test subject ID %s' % testgrps.RID.to_numpy(dtype=np.int)[subj_idx])

test_img = np.reshape(test_img, (1,)+ test_img.shape) # add first subj index again to mimic original array structure
test_orig = np.reshape(test_orig, (1,)+ test_orig.shape) # add first subj index again to mimic original array structure

# see https://github.com/albermax/innvestigate/blob/master/examples/notebooks/imagenet_compare_methods.ipynb for a list of alternative methods
methods = [ # tuple with method,     params,                  label
            #("deconvnet",            {},                      "Deconvnet"),
            #("guided_backprop",      {},                      "Guided Backprop"),
            #("deep_taylor.bounded",  {"low": -1, "high": 1},  "DeepTaylor"),
            #("input_t_gradient",     {},                      "Input * Gradient"),
            #("lrp.z",                {},                      "LRP-Z"),
            #("lrp.epsilon",          {"epsilon": 1},          "LRP-epsilon"),
            ("lrp.alpha_1_beta_0",   {"neuron_selection_mode":"index"},  "LRP-alpha1beta0"),
           #("lrp.alpha_1_beta_0",   {},                      "LRP-alpha1beta0"),
]

# create analyzer
analyzers = []
for method in methods:
#analyzer = innvestigate.create_analyzer("deep_taylor.bounded", model_wo_softmax, **params )
  analyzer = innvestigate.create_analyzer(method[0], model_wo_softmax, **method[1])
  # Some analyzers require training.
  analyzer.fit(test_img, batch_size=30, verbose=1)
  analyzers.append(analyzer)

from PIL import Image
from matplotlib import cm

# map: overlay
# rescale_threshold: max value to be plotted, larger values will be set to this value; 
#   corresponding to vmax in plt.imshow; vmin=-vmax used here
# alpha_threshold: absolute intensities below this value are removed from the plot; 
#   applied to given map rescaled to [-1, 1] so that negative activation is kept if present
# overlay_alpha: alpha value for final result image, applied to all pixels being displayed
def prepare_overlay( map, rescale_threshold = 3, alpha_treshold = 0.5, overlay_alpha = 0.5):
  #map = a[:,:, row * jump]
  map[map >= rescale_threshold] = rescale_threshold   #map[map >= 100] = 100 # thresholding
  map[map <= -rescale_threshold] = -rescale_threshold #map[map <= -100] = -100
  alpha_mask = map/rescale_threshold # range -1 to 1 float
  map = map/rescale_threshold/2 + 0.5 # range 0-1 float
  map = np.uint8(cm.jet(map) * 255) # cm translates range 0 - 255 uint to rgba array
  alpha_mask[np.abs(alpha_mask) < alpha_treshold] = 0 # completely hide low values
  alpha_mask[np.abs(alpha_mask) >= alpha_treshold] = overlay_alpha # final transparency of visible content
  map[:,:,3] = np.uint8(alpha_mask * 255) # replace alpha channel (fourth dim) with calculated values
  rgba_map = Image.fromarray(map)
  #plt.figure()
  #plt.imshow((test_orig[0, :,:, row*jump, 0]), cmap='gray')
  #plt.imshow(rgba_map)
  return rgba_map

import scipy
# display smoothed maps
for method,analyzer in zip(methods, analyzers):
  a = np.reshape(analyzer.analyze(test_img, neuron_selection=1), test_img.shape[1:4])
  
  #a = np.reshape(analyzer.analyze(test_img), test_img.shape[1:4])
  
  a = scipy.ndimage.filters.gaussian_filter(a, sigma=0.8) # smooth activity image
  scale = np.quantile(np.absolute(a), 0.99)
  if scale==0:
      scale = max(-np.amin(a), np.amax(a))
  #print(scale)
  a = (a/scale) # rescale range
  print(np.amax(a))
  #a = a.astype(int)
  #plt.figure()
  #plt.hist(np.reshape(a, -1), bins=50, range=(-10,10))
  #plt.gca().set_yscale('log')
  #plt.gca().set(title='Frequency Histogram', ylabel='Frequency');

  images_per_row = 10 # only ten linearly scliced out of 111 depth
  jump = (a.shape[2] // images_per_row)
  for row in range(images_per_row):
      plt.figure()
      plt.imshow((test_orig[0, :,:, row*jump, 0]), cmap='gray')
      #plt.figure()
      #plt.imshow((a[:,:, row * jump]), cmap='jet', vmin=-100, vmax=100, alpha=0.5)
      plt.imshow(prepare_overlay((a[:,:, row * jump]), alpha_treshold = 0.4, overlay_alpha = 0.7))
      plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)
      plt.gca().set(title=method[2], ylabel='Smoothed & thresholded', xlabel=('cor idx %d' % (row * jump)));

from keras.callbacks import CSVLogger
import datetime
#batch_sizes = [128] #16

#lr_n_decay = [[0.0001,0],[0.0001,0.001],[0.0001,0.0001],[0.001,0],[0.001,0.01],[0.001,0.001]]
#lr_n_decay = [[0.001,0.01]]

kernelsizes = [20] # varying only the kenel size of 1st conv layer
#dropout = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]
#dropout = [0.6,0.7,0.8]

#kernelsizes = [16] # varying only the kenel size of 1st conv layer
#lr_n_decay = [[0.0001,0],[0.0001,0.001],[0.0001,0.002],[0.0001,0.003],[0.0001,0.004],[0.0001,0.005],[0.0001,0.006]]

# Split data into training/validation and holdout test data
from sklearn.model_selection import StratifiedKFold,train_test_split
import numpy as np
from keras.utils import to_categorical
labels = to_categorical(np.asarray(labels)) # use grps to access original labels
acc_test, auc_test, auc_AD, auc_MCI = [], [], [], []
#skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)

pl_loss=[]
pl_val_loss=[]
pl_acc=[]
pl_val_acc=[]

#idx = np.asarray(range(numfiles))
#tmp_Y = np.asarray(grps.iloc[:, 0])

#tmp_idX, test_idX = train_test_split(idx,test_size=0.1,random_state=2) # Splitting into test and tmp set , tmp set to be further split into train and valid
#tmp_Y = np.asarray(grps.iloc[tmp_idX, 0])
#%matplotlib inline
for ks in kernelsizes:
  pl_loss=[]
  pl_val_loss=[]
  pl_acc=[]
  pl_val_acc=[]
  cnt=1

  #NAME = "{}-batch_size-{}".format(batch_size, int(time.time()))
  #tboard_log_dir = os.path.join("logs",NAME)
  #tensorboard = TensorBoard(log_dir = tboard_log_dir)
  #print(batch_size)
  #for tmp_idX, test_idX in skf.split(X=images, y=grps.iloc[:, 0]): # split data as tenfold stratified cross-validation
      #tmp_Y = np.asarray(grps.iloc[tmp_idX, 0])
  train,test,train_Y,test_Y = train_test_split(images, grps.iloc[:, 0], test_size=0.1, stratify=grps.iloc[:, 0], random_state=2)

  skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # do 10 fold xval

  for train_idX,valid_idX in skf.split(train, train_Y): # split data as tenfold stratified cross-validation
      print("fold = ",cnt)
      print('valid index : ',valid_idX)
      print('train index: ',train_idX)
      #print('test index : ',test)
      #print('fold = ',skf.get_n_splits(tmp_idX,tmp_Y))
      cnt= cnt + 1